{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = 'data.txt'\n",
    "\n",
    "data = np.genfromtxt(filePath, delimiter=',', skip_header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_t = np.transpose(data)\n",
    "len(data_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# transpose data to get features in columns and samples in rows\n",
    "data_transpose = np.transpose(data)\n",
    "data_list = data_transpose.tolist()\n",
    "\n",
    "# now let's seperate data into \"White Light Frame (WL)\" and \"NBI Frame (NBI)\"\n",
    "# 1 for WL and 2 for NBI\n",
    "data_WL, data_NBI = [],[]\n",
    "for i in range(len(data_list)):\n",
    "    if data_list[i][1] == 1:\n",
    "        data_WL.append(data_list[i])\n",
    "    elif data_list[i][1] == 2:\n",
    "        data_NBI.append(data_list[i])\n",
    "\n",
    "# checking if the separation was done correctely\n",
    "print(False in [row[1]==1 for row in data_WL])  # should be False\n",
    "print(False in [row[1]==2 for row in data_NBI]) # should be False\n",
    "print((len(data_WL)+len(data_NBI))==len(data_list)) # should be True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating features and targets out of data_WL and data_NBI\n",
    "fea_WL = [row[2:] for row in data_WL]\n",
    "class_WL = [row[0] for row in data_WL]\n",
    "\n",
    "fea_NBI = [row[2:] for row in data_NBI]\n",
    "class_NBI = [row[0] for row in data_NBI]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7368421052631579\n",
      "[[7 0 2]\n",
      " [0 2 1]\n",
      " [1 1 5]]\n"
     ]
    }
   ],
   "source": [
    "# importing necessary libraries\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X -> features, y -> label\n",
    "X = fea_WL\n",
    "y = class_WL\n",
    "\n",
    "# dividing X, y into train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "# training a linear SVM classifier\n",
    "from sklearn.svm import SVC\n",
    "svm_model_linear = SVC(kernel = 'linear', C = 1).fit(X_train, y_train)\n",
    "svm_predictions = svm_model_linear.predict(X_test)\n",
    " \n",
    "# model accuracy for X_test \n",
    "accuracy = svm_model_linear.score(X_test, y_test)\n",
    "print(accuracy)\n",
    " \n",
    "# creating a confusion matrix\n",
    "cm = confusion_matrix(y_test, svm_predictions)\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.618 (0.486)\n"
     ]
    }
   ],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# X -> features, y -> label\n",
    "X = fea_WL\n",
    "y = class_WL\n",
    "# create loocv procedure\n",
    "cv = LeaveOneOut()\n",
    "# create model\n",
    "model = RandomForestClassifier(random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.618 (0.486)\n"
     ]
    }
   ],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# X -> features, y -> label\n",
    "X = fea_WL\n",
    "y = class_WL\n",
    "# define the model\n",
    "model = AdaBoostClassifier()\n",
    "# create loocv procedure\n",
    "cv = LeaveOneOut()\n",
    "# # evaluate the model\n",
    "# cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (68, 698) (68,) Test (8, 698) (8,)\n",
      "Meta  (68, 27) (68,)\n",
      "LogisticRegression: 50.000\n",
      "DecisionTreeClassifier: 75.000\n",
      "SVC: 62.500\n",
      "GaussianNB: 25.000\n",
      "KNeighborsClassifier: 75.000\n",
      "AdaBoostClassifier: 62.500\n",
      "BaggingClassifier: 75.000\n",
      "RandomForestClassifier: 75.000\n",
      "ExtraTreesClassifier: 50.000\n",
      "Super Learner: 75.000\n"
     ]
    }
   ],
   "source": [
    "from numpy import hstack\n",
    "from numpy import vstack\n",
    "from numpy import asarray\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    " \n",
    "# create a list of base-models\n",
    "def get_models():\n",
    "\tmodels = list()\n",
    "\tmodels.append(LogisticRegression(solver='liblinear'))\n",
    "\tmodels.append(DecisionTreeClassifier())\n",
    "\tmodels.append(SVC(gamma='scale', probability=True))\n",
    "\tmodels.append(GaussianNB())\n",
    "\tmodels.append(KNeighborsClassifier())\n",
    "\tmodels.append(AdaBoostClassifier())\n",
    "\tmodels.append(BaggingClassifier(n_estimators=10))\n",
    "\tmodels.append(RandomForestClassifier(n_estimators=10))\n",
    "\tmodels.append(ExtraTreesClassifier(n_estimators=10))\n",
    "\treturn models\n",
    " \n",
    "# collect out of fold predictions form k-fold cross validation\n",
    "def get_out_of_fold_predictions(X, y, models):\n",
    "\tmeta_X, meta_y = list(), list()\n",
    "\t# define split of data\n",
    "\tkfold = KFold(n_splits=10, shuffle=True)\n",
    "\t# enumerate splits\n",
    "\tfor train_ix, test_ix in kfold.split(X):\n",
    "\t\tfold_yhats = list()\n",
    "\t\t# get data\n",
    "\t\ttrain_X, test_X = X[train_ix], X[test_ix]\n",
    "\t\ttrain_y, test_y = y[train_ix], y[test_ix]\n",
    "\t\tmeta_y.extend(test_y)\n",
    "\t\t# fit and make predictions with each sub-model\n",
    "\t\tfor model in models:\n",
    "\t\t\tmodel.fit(train_X, train_y)\n",
    "\t\t\tyhat = model.predict_proba(test_X)\n",
    "\t\t\t# store columns\n",
    "\t\t\tfold_yhats.append(yhat)\n",
    "\t\t# store fold yhats as columns\n",
    "\t\tmeta_X.append(hstack(fold_yhats))\n",
    "\treturn vstack(meta_X), asarray(meta_y)\n",
    " \n",
    "# fit all base models on the training dataset\n",
    "def fit_base_models(X, y, models):\n",
    "\tfor model in models:\n",
    "\t\tmodel.fit(X, y)\n",
    " \n",
    "# fit a meta model\n",
    "def fit_meta_model(X, y):\n",
    "\tmodel = LogisticRegression(solver='liblinear')\n",
    "\tmodel.fit(X, y)\n",
    "\treturn model\n",
    " \n",
    "# evaluate a list of models on a dataset\n",
    "def evaluate_models(X, y, models):\n",
    "\tfor model in models:\n",
    "\t\tyhat = model.predict(X)\n",
    "\t\tacc = accuracy_score(y, yhat)\n",
    "\t\tprint('%s: %.3f' % (model.__class__.__name__, acc*100))\n",
    " \n",
    "# make predictions with stacked model\n",
    "def super_learner_predictions(X, models, meta_model):\n",
    "\tmeta_X = list()\n",
    "\tfor model in models:\n",
    "\t\tyhat = model.predict_proba(X)\n",
    "\t\tmeta_X.append(yhat)\n",
    "\tmeta_X = hstack(meta_X)\n",
    "\t# predict\n",
    "\treturn meta_model.predict(meta_X)\n",
    " \n",
    "# X -> features, y -> label\n",
    "X = np.array(fea_NBI)\n",
    "y = np.array(class_NBI)\n",
    "# split\n",
    "X, X_val, y, y_val = train_test_split(X, y, test_size=0.1)\n",
    "print('Train', X.shape, y.shape, 'Test', X_val.shape, y_val.shape)\n",
    "# get models\n",
    "models = get_models()\n",
    "# get out of fold predictions\n",
    "meta_X, meta_y = get_out_of_fold_predictions(X, y, models)\n",
    "print('Meta ', meta_X.shape, meta_y.shape)\n",
    "# fit base models\n",
    "fit_base_models(X, y, models)\n",
    "# fit the meta model\n",
    "meta_model = fit_meta_model(meta_X, meta_y)\n",
    "# evaluate base models\n",
    "evaluate_models(X_val, y_val, models)\n",
    "# evaluate meta model\n",
    "yhat = super_learner_predictions(X_val, models, meta_model)\n",
    "print('Super Learner: %.3f' % (accuracy_score(y_val, yhat) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# X -> features, y -> label\n",
    "X = np.array(fea_WL)\n",
    "y = np.array(class_WL)\n",
    "\n",
    "similarity_matrix = cosine_similarity(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.93607026, 0.97028607, ..., 0.99687239, 0.98485682,\n",
       "        0.91366254],\n",
       "       [0.93607026, 1.        , 0.98597573, ..., 0.95746692, 0.9496139 ,\n",
       "        0.98196557],\n",
       "       [0.97028607, 0.98597573, 1.        , ..., 0.98576319, 0.96913295,\n",
       "        0.96665076],\n",
       "       ...,\n",
       "       [0.99687239, 0.95746692, 0.98576319, ..., 1.        , 0.98455307,\n",
       "        0.93653564],\n",
       "       [0.98485682, 0.9496139 , 0.96913295, ..., 0.98455307, 1.        ,\n",
       "        0.93032114],\n",
       "       [0.91366254, 0.98196557, 0.96665076, ..., 0.93653564, 0.93032114,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# create nx graph from sim matrix\n",
    "G = nx.to_networkx_graph(similarity_matrix)\n",
    "\n",
    "nx.draw(G,with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a5e9bb5c8109d9c89e3540dfae7ab7a0527331017fc9eeaf677245afa113f1a5"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
